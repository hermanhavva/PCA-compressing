# Теорія
1. **Власні значення** певної матриці **А** обчислюються за допомогою характеристичного рівняння
   **|A-λ*E|**. Після знаходження власних значень `λ1, λ2...λn` можемо знайти **власні вектори** - такі вектори, котрі 
не змінюють своє положення при множенні на матрицю лін. трансформації зліва(тільки змінюють свою норму). Важливо, що 
одному власному вектору відповідає одне власне значення, хоча одному власному значенню иож відповідати безліч власних
векторів.  
2. **Власні вектори симетричних матриць**, 
- якщо вл. вектори вони належать попарно різним власним значенням - то вони ортогональні. 
- **Формула** є **спектральний розкладом** `A=QΛQ^Т` , де **Q** - ортогональна матриця, що містить власні вектори **А**, 
а **Λ** - діагональна матриця шо містить 
власні значення **А**. 
- Будь-яку симетричну матрицю можна діагоналізувати у власному ортогональному базисі `Q^T*A*Q=D` 
3. Недоліки **РСА**:
- Лінійність: PCA базується на лінійних припущеннях і не може захоплювати нелінійні зв'язки між змінними.
- Чутливість до масштабу: PCA є чутливим до масштабування змінних. Змінні з великими масштабами можуть домінувати в аналізі.
- Втрати інформації: При зменшенні розмірності деяка інформація може бути втрачена, що може вплинути на результати подальшого аналізу.
- Шум та аномалії: PCA може бути чутливим до шуму та аномалій у даних, що може призвести до ненадійних компонентів.
- Інтерпретація компонент: Головні компоненти можуть бути важко інтерпретувати, оскільки вони є лінійними комбінаціями початкових змінних.
 **Шляхи подолання недоліків:**
- Використання нелінійних методів зменшення розмірності 
- Масштабування даних: Перед застосуванням PCA важливо нормалізувати або стандартизувати змінні, щоб кожна з них мала однаковий вплив на результати аналізу.
- Видалення шуму та аномалій (регуляризовані версії PCA, такі як Sparse PCA або Robust PCA) краще працюють з аномаліями
- Крос-валідація: Використання крос-валідації для вибору оптимальної кількості компонент може допомогти знайти баланс між зменшенням розмірності та збереженням достатньої кількості інформації
4.
# Чому діагональну матрицю використовують у криптографії
- Легко рахувати обернену до неї, легко рахувати детермінант -> 
- Менша вирогідність помилок
# Як ії використовують? 
